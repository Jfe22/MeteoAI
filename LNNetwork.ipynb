{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebaab40-ec3b-4096-90f4-5b6d7f915557",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Imports, Functions and Variables\n",
    "Ricardo Crespo, Miguel Melro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938a4c4e-f7f0-45db-81a3-27875048479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook can  be used. Restart Kernel and run.\n",
    "# This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs (CC BY-NC-ND 4.0) License\n",
    "# and is subject to patent application regulations.\n",
    "# Author: João da Silva Pereira (joao.pereira@ipleiria.pt)\n",
    "\n",
    "# Clear all variables from the workspace\n",
    "#%reset -f\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.layers import Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import reshape\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "# FOR LNN/LTC\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "WMethod = 1 # see the argument of getInitialier(). 1 is PSJ and <> of 1 is a random method. Should be a number between 1 and 15.\n",
    "NLayer_ini = 1 # Inicial hidden layer number \n",
    "NLayermax = 2 #show -1 HLayers than this number\n",
    "deltaNLayer = 1 # Delta increment for hidden layers\n",
    "NEpochs = 100 # Epochs number alterado de 11 para 100\n",
    "n_folds = 10  # Folders number. Should be higher than 2 \n",
    "N_neuronio = 64 # Neurons number\n",
    "Batch_len = 5   # Batch Size number\n",
    "\n",
    "Xinput = 10000 # Input number \n",
    "Youtput = 3  # Output number\n",
    "Namostra= 16  # Samples number for test validation\n",
    "\n",
    "#Funções adicionadas para o data fetch do dataset de percipitação\n",
    "def count_how_many_occurrences_of_each_value(arr):\n",
    "    arr = np.ravel(arr)\n",
    "    counts = {}\n",
    "    for num in arr:\n",
    "        if num in counts:\n",
    "            counts[num] += 1\n",
    "        else:\n",
    "            counts[num] = 1\n",
    "    ordered_counts = OrderedDict(sorted(counts.items()))\n",
    "    return ordered_counts\n",
    "\n",
    "def list_dataset_folders():\n",
    "    folders = []\n",
    "    folder_path = os.getcwd()\n",
    "    folder_path = os.path.join(folder_path, \"datasets\")\n",
    "    for name in os.listdir(folder_path):\n",
    "        if os.path.isdir(os.path.join(folder_path, name)):\n",
    "            if name.endswith(\"dataset\"):\n",
    "                folders.append(name)\n",
    "    return folders\n",
    "\n",
    "\n",
    "def resize_image(image_path):\n",
    "    # Open the image file\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Resize the image to 100x100 pixels\n",
    "    resized_image = image.resize((100, 100))\n",
    "\n",
    "    # Return the resized image\n",
    "    return resized_image\n",
    "\n",
    "#Ids das estações\n",
    "ids = np.array([1240610, 1210718, 1210702, 1200562, 6212124, 1200575, 1200570, 1200571, 1240903, 1210734, 1210707, 1200558, 1200554, 1210683, 7240919, 1210770, 1240566, 1240675])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458c95a-5bd8-4071-b7dd-acb88f95f111",
   "metadata": {},
   "source": [
    "# Change Normalization Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb4cb1-fc30-4f03-a248-42c3be9e492c",
   "metadata": {},
   "source": [
    "# Data Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cb86fb-c659-4b7f-a13a-aaa293010c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many of each value in train:\n",
      "OrderedDict([(np.float64(0.0), 13), (np.float64(1.0), 13), (np.float64(2.0), 13)])\n",
      "How many of each value in validation:\n",
      "OrderedDict([(np.float64(0.0), 7), (np.float64(1.0), 7), (np.float64(2.0), 4)])\n"
     ]
    }
   ],
   "source": [
    "#Data treatment para o dataset de percipitação\n",
    "limit_dictionary = {key: 20 for key in range(101)} #para usar mais imagens de chuva, aumentar o valor do limite para cada categoria \n",
    "time_difference = 1 #diferença entre a hora da imagem e a hora do valor da chuva\n",
    "\n",
    "image_size = 100 \n",
    "counter_dictionary = {key: 0 for key in range(101)}\n",
    "specific_datasets = list_dataset_folders()\n",
    "data_array = np.empty(0)\n",
    "images_array = np.empty((0, image_size, image_size, 4))\n",
    "\n",
    "np.random.shuffle(specific_datasets)\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "#for dataset in specific_datasets:\n",
    "for stationID in ids:\n",
    "    currentDir = 'datasets/dataset/precipitation/'+str(stationID)+'.json'\n",
    "    with open(currentDir) as f:\n",
    "        # Load the JSON data\n",
    "        data = json.load(f)\n",
    "    f.close()\n",
    "    for date, hours in data.items():\n",
    "        for hour in hours:\n",
    "            hora_da_imagem = datetime.strptime(hour, \"%H:%M\")\n",
    "            # Subtract one hour\n",
    "            hora_da_imagem = hora_da_imagem - timedelta(hours=time_difference)\n",
    "            # Convert back to string\n",
    "            hora_da_imagem = hora_da_imagem.strftime(\"%H:%M\")\n",
    "            variable = date + 'T' + hora_da_imagem.replace(':', '') + '.png'  \n",
    "            value = data[date][hour]\n",
    "    \n",
    "            if not os.path.exists('datasets/dataset/images/'+str(stationID)+'/'+date+'/'+variable):\n",
    "                continue  # Skip the current iteration if the image doesn't exist\n",
    "            if counter_dictionary[value] < limit_dictionary[value]:\n",
    "                counter_dictionary[value] += 1\n",
    "                data_array = np.append(data_array, value)\n",
    "                image = resize_image('datasets/dataset/images/'+str(stationID)+'/'+date+'/'+variable)\n",
    "                img_np = np.array(image)\n",
    "                image.close()\n",
    "                images_array = np.append(images_array,[img_np], axis=0) #NAO TIRAR PARENTESIS RETOS!!!!!!!!!!!!!!\n",
    "                \n",
    "                \"\"\"# Duplicar imagens da classe(s) minoritária(s) (classe 1 e 2) Abordagem 3\n",
    "                if value == 2:\n",
    "                    for _ in range(177):  # Ajustar o número de duplicações para balancear com a classe 0\n",
    "                        data_array = np.append(data_array, value)\n",
    "                        images_array = np.append(images_array, [img_np], axis=0)\n",
    "                elif value == 1:\n",
    "                    for _ in range(50):  # Ajustar o número de duplicações para balancear com a classe 0\n",
    "                        data_array = np.append(data_array, value)\n",
    "                        images_array = np.append(images_array, [img_np], axis=0)\"\"\"\n",
    "\n",
    "perm = np.random.permutation(len(data_array))\n",
    "images_array, data_array = images_array[perm], data_array[perm]\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images_array, data_array, test_size=0.3, random_state=2) \n",
    "\n",
    "train_images = tf.keras.utils.normalize(train_images, axis=1)\n",
    "test_images = tf.keras.utils.normalize(test_images, axis=1)\n",
    "\n",
    "#Print Imagem\n",
    "plt.imshow(train_images[0])\n",
    "plt.axis('off')  # Disable axis\n",
    "plt.show()  \n",
    "\n",
    "print(\"How many of each value in train:\")\n",
    "print(count_how_many_occurrences_of_each_value(train_labels))\n",
    "print(\"How many of each value in validation:\")\n",
    "print(count_how_many_occurrences_of_each_value(test_labels))\n",
    "\n",
    "# Initialize inputs tensors/arrays for Training and testing\n",
    "x_train = train_images[:6*Namostra, :100, :100] \n",
    "x_test = test_images[:Namostra, :100, :100] \n",
    "\n",
    "# Initialize ouputs tensors/arrays for Training and testing\n",
    "y_train = train_labels[:6*Namostra]\n",
    "y_test = test_labels[:Namostra]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2f94c-9121-4401-80d2-072fd1eccdca",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876790e5-aaac-4d03-9edc-f4652cc4116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:47:20.339912: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-03-25 17:47:20.340034: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2025-03-25 17:47:20.340041: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742924840.340451  564505 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1742924840.340779  564505 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini Weight initialization\n",
      "Method PSJ\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 40000 into shape (5,100,100,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 239\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Create 3 initializer that will train the model\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIni Weight initialization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m initializer1, initializer2, initializer10 \u001b[38;5;241m=\u001b[39m \u001b[43mgetInitialier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWMethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd Weight initialization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Writer do xls to save data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 176\u001b[0m, in \u001b[0;36mgetInitialier\u001b[0;34m(WMethod)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m WMethod\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod PSJ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m     PSJ \u001b[38;5;241m=\u001b[39m \u001b[43mPSJmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(PSJ)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mprint\u001b[39m(PSJ\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[3], line 124\u001b[0m, in \u001b[0;36mPSJmethod\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m soma \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m cont \n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Call the prediction function inside the loop\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m soma_r \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m#print(soma.shape)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(soma)\n",
      "Cell \u001b[0;32mIn[3], line 108\u001b[0m, in \u001b[0;36mpredict_with_model\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_with_model\u001b[39m(image):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model1\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 40000 into shape (5,100,100,4)"
     ]
    }
   ],
   "source": [
    "# Initialize the model outside the loop\n",
    "# ======================== OLD CNN ====================================== #\n",
    "#model1 = Sequential()\n",
    "#model1.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last',\n",
    "#                input_shape=(100,100,4)))\n",
    "#model1.add(BatchNormalization())\n",
    "#model1.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n",
    "#model1.add(BatchNormalization())\n",
    "#model1.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid' ))\n",
    "#model1.add(Dropout(0.35)) #alterado de 0.35 para 0.3\n",
    "#model1.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n",
    "#model1.add(BatchNormalization())\n",
    "#model1.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu', data_format='channels_last'))\n",
    "#model1.add(BatchNormalization())\n",
    "#model1.add(MaxPooling2D(pool_size=(2, 2), padding='valid', strides=2))\n",
    "#model1.add(Dropout(0.0035)) \n",
    "#model1.add(Flatten(name='flatten')) \n",
    "#model1.summary()\n",
    "# ======================== OLD CNN ====================================== #\n",
    "\n",
    "# ======================== NEW LTC + CNN ====================================== #\n",
    "# Custom Liquid Time-Constant (LTC) Layer\n",
    "class LTCLayer(layers.Layer):\n",
    "    def __init__(self, units, dt=1.0, **kwargs):\n",
    "        super(LTCLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dt = dt  # Time interval between radar frames (e.g., 10 minutes)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Input shape: (batch, timesteps, features)\n",
    "        self.w_input = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        trainable=True)\n",
    "        self.w_state = self.add_weight(shape=(self.units, self.units),\n",
    "                                        initializer='orthogonal',\n",
    "                                        trainable=True)\n",
    "        self.tau = self.add_weight(shape=(self.units,),\n",
    "                                    initializer=tf.keras.initializers.Constant(1.0),\n",
    "                                    trainable=True)  # Time constants\n",
    "        self.bias = self.add_weight(shape=(self.units,),\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, timesteps, features)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        timesteps = inputs.shape[1]\n",
    "        \n",
    "        # Initial state (zeros)\n",
    "        state = tf.zeros((batch_size, self.units))\n",
    "        outputs = []\n",
    "\n",
    "        # Iterate over timesteps (discretized LTC dynamics)\n",
    "        for t in range(timesteps):\n",
    "            x = inputs[:, t, :]  # Current timestep features\n",
    "            # Compute input modulation\n",
    "            modulated_input = tf.matmul(x, self.w_input)\n",
    "            # Update state using Euler-like ODE step\n",
    "            state_derivative = tf.nn.tanh(modulated_input + tf.matmul(state, self.w_state) + self.bias - state) / self.tau\n",
    "            state = state + self.dt * state_derivative\n",
    "            outputs.append(state)\n",
    "        \n",
    "        # Return final state (or sequence if return_sequences=True)\n",
    "        return outputs[-1]  # Shape: (batch, units)\n",
    "\n",
    "# Build Hybrid Model\n",
    "def build_hybrid_model(input_shape, n_classes, timesteps=5):\n",
    "    inputs = layers.Input(shape=(timesteps, 100, 100, 4))\n",
    "    \n",
    "    # Spatial Feature Extractor (CNN)\n",
    "    cnn = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.35),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    # Process each timestep with CNN\n",
    "    x = layers.TimeDistributed(cnn)(inputs)  # Output shape: (batch, timesteps, 128)\n",
    "    \n",
    "    # Temporal Dynamics (Liquid Neural Network)\n",
    "    x = LTCLayer(units=64, dt=1.0)(x)  # Custom LTC layer\n",
    "    x = layers.Dropout(0.35)(x)\n",
    "    \n",
    "    # Multi-Time Horizon Prediction\n",
    "    outputs = []\n",
    "    for horizon in [1, 2, 3]:  # Predict 1h, 2h, 3h ahead\n",
    "        # Shared dense layers for all horizons\n",
    "        y = layers.Dense(64, activation='relu')(x)\n",
    "        y = layers.Dense(n_classes, activation='softmax', name=f'output_{horizon}h')(y)\n",
    "        outputs.append(y)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Initialize Model\n",
    "model1 = build_hybrid_model(input_shape=(5, 100, 100, 4), n_classes=3)  # Example: 3 rain classes\n",
    "# ======================== NEW LTC + CNN ====================================== #\n",
    "\n",
    "# Define the prediction function outside the loop\n",
    "def predict_with_model(image):\n",
    "    return model1.predict(image.reshape(5, 100, 100, 4), verbose=0) \n",
    "    \n",
    "#function that implements the PSJ method for the 1st Deep Learning layer (João Silva Pereira)\n",
    "def PSJmethod():\n",
    "    z=np.zeros((N_neuronio, Xinput*4))     # reshaping here\n",
    "    for i in range(0, N_neuronio):\n",
    "        somas = []\n",
    "        if i < Youtput:\n",
    "            # arranjar forma de mandar 5 imagens?\n",
    "            while(len(somas) < 5):\n",
    "                soma = np.zeros([100,100,4]) \n",
    "                cont = 0\n",
    "                for n in range(0, len(x_train)):\n",
    "                    if y_train[n] == np.mod(i,Youtput):\n",
    "                        soma += x_train[n]\n",
    "                        cont += 1\n",
    "                soma /= cont \n",
    "\n",
    "                somas.append(soma)\n",
    "                i=i+1\n",
    "\n",
    "            # Call the prediction function inside the loop\n",
    "            soma_r = predict_with_model(soma)\n",
    "            \n",
    "            if i == 2:\n",
    "                #print(soma.shape)\n",
    "                print(soma)\n",
    "                # Plot the array as an image\n",
    "                plt.imshow(soma)\n",
    "                plt.axis('off')  # Disable axis\n",
    "                plt.show()\n",
    "\n",
    "                print(soma_r.shape)\n",
    "                #print(soma)\n",
    "                # Plot the array as an image\n",
    "                plt.imshow(soma_r.reshape(200,200))\n",
    "                plt.axis('off')  # Disable axis\n",
    "                plt.show()   \n",
    "            \n",
    "            soma = reshape(soma_r, [100*100*4])      # reshaping here \n",
    "            s = np.array(soma,dtype=\"cfloat\")\n",
    "            s = np.fft.fft(s)\n",
    "            s = np.cos(np.angle(s))+np.sin(np.angle(s))*1j\n",
    "            s = np.fft.ifft(s)\n",
    "            z[i] = s.real\n",
    "        else:\n",
    "            z[i] = z[np.mod(i,Youtput)]\n",
    "    return z\n",
    "\n",
    "#function that implements the PSJ method for the outher Deep Learning layers (João Silva Pereira)\n",
    "def iniL2(N_neur, inp):\n",
    "    LastLayerANN=np.zeros((N_neur, inp))\n",
    "    for r in range(0,N_neur):\n",
    "        if np.mod(r,inp)==0:\n",
    "            for k in range(0,inp):  \n",
    "                for s in range(0,inp): \n",
    "                    if k==s:\n",
    "                        if (r+k)<N_neur:\n",
    "                            LastLayerANN[r+k][s]=1\n",
    "    return LastLayerANN\n",
    "\n",
    "   \n",
    "# Initialize the seeds within tensorflow for results to be reproducible\n",
    "# Seeds for NumPy and TensorFlow\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "# Settings for reproducibility (this may affect performance)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Function that initialize the Weights of the Deep Learning Network for 15 different methods\n",
    "def getInitialier(WMethod):\n",
    "    if WMethod==1:\n",
    "        print(\"Method PSJ\")\n",
    "        PSJ = PSJmethod()\n",
    "        print(PSJ)\n",
    "        print(PSJ.shape)\n",
    "        return tf.constant_initializer(PSJ), tf.constant_initializer(iniL2(N_neuronio, N_neuronio)), tf.keras.initializers.Ones()\n",
    "    if WMethod==2:\n",
    "        print(\"Method GlorotNormal\")\n",
    "        iniM = tf.keras.initializers.GlorotNormal(seed=seed_value)\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==3:\n",
    "        print(\"Method GlorotUniform\")\n",
    "        iniM = tf.keras.initializers.GlorotUniform(seed=seed_value)\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==4:\n",
    "        print(\"Method HeNormal\")\n",
    "        iniM = tf.keras.initializers.HeNormal(seed=seed_value)\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==5:\n",
    "        print(\"Method HeUniform\")\n",
    "        iniM = tf.keras.initializers.HeUniform(seed=seed_value)\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==6:\n",
    "        print(\"Method LecunNormal\")\n",
    "        iniM = tf.keras.initializers.LecunNormal(seed=seed_value)\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==7:\n",
    "        print(\"Method LecunUniform\")\n",
    "        iniM = tf.keras.initializers.LecunUniform(seed=seed_value) \n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==8:\n",
    "        print(\"Method Orthogonal\")\n",
    "        iniM = tf.keras.initializers.Orthogonal()\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==9:\n",
    "        print(\"Method RandomNormal\")\n",
    "        iniM = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==10:\n",
    "        print(\"Method RandomUniform\")\n",
    "        iniM = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==11:\n",
    "        print(\"Method Identity\")\n",
    "        iniM = tf.keras.initializers.Identity()\n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==12:\n",
    "        print(\"Method TruncatedNormal\")\n",
    "        iniM = tf.keras.initializers.TruncatedNormal(mean=0., stddev=0.5) \n",
    "        return iniM, iniM, iniM\n",
    "    if WMethod==13:\n",
    "        print(\"Method VarianceScaling\")\n",
    "        iniM = tf.keras.initializers.VarianceScaling(scale=0.1, mode='fan_in', distribution='uniform') \n",
    "        return iniM, iniM, iniM \n",
    "    if WMethod==14:\n",
    "        print(\"Method Zeros\")\n",
    "        iniM = tf.keras.initializers.Zeros()\n",
    "        return iniM, iniM, iniM \n",
    "    if WMethod==15:\n",
    "        print(\"Method Ones\")\n",
    "        iniM = tf.keras.initializers.Ones()\n",
    "        return iniM, iniM, iniM \n",
    "\n",
    "# Create 3 initializer that will train the model\n",
    "print(\"Ini Weight initialization\")\n",
    "initializer1, initializer2, initializer10 = getInitialier(WMethod)\n",
    "print(\"End Weight initialization\")\n",
    "\n",
    "# Writer do xls to save data\n",
    "xlsFileName = 'Train_method'+str(WMethod)+' N'+str(N_neuronio)+\" MaxHiddenLayer\"+str((NLayermax-1))+\" \"+\" MaxFolds\"+str(n_folds)+\" \"+ datetime.now().strftime(\"%Y %m %d-%H%M%S\")+'.xlsx'\n",
    "writer = pd.ExcelWriter(xlsFileName, engine='openpyxl')\n",
    "\n",
    "# Create empty arrays\n",
    "HidenLayeres =[]\n",
    "train_accuracies_=[]\n",
    "losses_=[]\n",
    "accuracies_=[]\n",
    "val_losses_=[] \n",
    "precisions_=[]  \n",
    "val_precisions_=[]      \n",
    "recalles_=[]     \n",
    "val_recalles_=[] \n",
    "\n",
    "# Start the program for different configurations\n",
    "try:   \n",
    "    # Configuration the the hidden layers\n",
    "    for n_HL in range(NLayer_ini, NLayermax, deltaNLayer): \n",
    "        NLayer=n_HL\n",
    "        print(\"\")\n",
    "        print(\"Hidden Layer = \", NLayer)\n",
    "        accuracies = []\n",
    "        train_accuracies = []\n",
    "        precisions = []\n",
    "        val_precisions = []\n",
    "        losses =[]\n",
    "        val_losses = []\n",
    "        recalles =[]\n",
    "        val_recalles = []\n",
    "        # Configuration of the different folds\n",
    "        for fold in range(0, n_folds):\n",
    "            fold += 1\n",
    "            print(f\"\\nFold {fold}\")\n",
    "            print(\"\")\n",
    "                \n",
    "            x_train_fold = x_train.reshape(-1, 100, 100, 4) \n",
    "            y_train_fold =  y_train    \n",
    "            x_val_fold = x_test\n",
    "            y_val_fold = y_test\n",
    "            \n",
    "            y_train_one_hot = to_categorical(y_train_fold, num_classes=Youtput) \n",
    "            y_val_one_hot = to_categorical(y_val_fold, num_classes=Youtput) \n",
    "            \n",
    "            #Callback to save the Keras model or model weights at some frequency\n",
    "            model_checkpoint = callbacks.ModelCheckpoint(filepath=\"model_weights_by_hour/hour_difference_\" + str(time_difference) + \".weights.h5\", monitor='val_accuracy', save_best_only=True, verbose=1, mode='max', save_weights_only=True)\n",
    "\n",
    "            # Configuration of the Augmentation method\n",
    "            datagen = ImageDataGenerator(\n",
    "                rotation_range=5, \n",
    "                width_shift_range=0.02,\n",
    "                height_shift_range=0.02,\n",
    "                shear_range=0.02,\n",
    "                zoom_range=0,\n",
    "                fill_mode='nearest'\n",
    "            )\n",
    "\n",
    "            # ======================== OLD CNN ====================================== #\n",
    "            # Deep Learning Network configuration\n",
    "            #model = tf.keras.models.Sequential()   \n",
    "            \n",
    "            #model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last',\n",
    "                            #input_shape=(100,100,4)))\n",
    "            #model.add(BatchNormalization())\n",
    "            #model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n",
    "            #model.add(BatchNormalization())\n",
    "            #model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid' ))\n",
    "            #model.add(Dropout(0.35)) \n",
    "            #model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n",
    "            #model.add(BatchNormalization())\n",
    "            #model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu', data_format='channels_last'))\n",
    "            #model.add(BatchNormalization())\n",
    "            #model.add(MaxPooling2D(pool_size=(2, 2), padding='valid', strides=2))\n",
    "            #model.add(Dropout(0.0035))  \n",
    "            #model.add(Flatten(name='flatten'))           \n",
    "            #Bias_ini = 'zeros'\n",
    "            #model.add(layers.Dense(N_neuronio, activation='relu', kernel_initializer=initializer1, bias_initializer=Bias_ini))\n",
    "            # Configuration of the quantity of Hidden Layers\n",
    "            #for f in range(1, NLayer+1):\n",
    "            #    model.add(layers.Dense(N_neuronio, activation='relu', kernel_initializer=initializer2, bias_initializer=Bias_ini))\n",
    "            #model.add(layers.BatchNormalization())  \n",
    "            #model.add(tf.keras.layers.Dense(Youtput, activation='softmax', kernel_initializer=initializer10))\n",
    "\n",
    "            # Compile Model\n",
    "            #model.compile(optimizer=keras.optimizers.Adamax(learning_rate=0.0055), \n",
    "            #            loss='categorical_crossentropy',   \n",
    "            #            #metrics=['accuracy'])  \n",
    "            #            metrics=['accuracy', 'precision', 'recall'])  \n",
    "            # ======================== OLD CNN ====================================== #\n",
    "\n",
    "\n",
    "            # ======================== NEW LTC + CNN ====================================== #\n",
    "            # Custom Liquid Time-Constant (LTC) Layer\n",
    "            class LTCLayer(layers.Layer):\n",
    "                def __init__(self, units, dt=1.0, **kwargs):\n",
    "                    super(LTCLayer, self).__init__(**kwargs)\n",
    "                    self.units = units\n",
    "                    self.dt = dt  # Time interval between radar frames (e.g., 10 minutes)\n",
    "\n",
    "                def build(self, input_shape):\n",
    "                    # Input shape: (batch, timesteps, features)\n",
    "                    self.w_input = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                                    initializer='glorot_uniform',\n",
    "                                                    trainable=True)\n",
    "                    self.w_state = self.add_weight(shape=(self.units, self.units),\n",
    "                                                    initializer='orthogonal',\n",
    "                                                    trainable=True)\n",
    "                    self.tau = self.add_weight(shape=(self.units,),\n",
    "                                                initializer=tf.keras.initializers.Constant(1.0),\n",
    "                                                trainable=True)  # Time constants\n",
    "                    self.bias = self.add_weight(shape=(self.units,),\n",
    "                                                initializer='zeros',\n",
    "                                                trainable=True)\n",
    "                    super().build(input_shape)\n",
    "\n",
    "                def call(self, inputs):\n",
    "                    # inputs shape: (batch, timesteps, features)\n",
    "                    batch_size = tf.shape(inputs)[0]\n",
    "                    timesteps = inputs.shape[1]\n",
    "                    \n",
    "                    # Initial state (zeros)\n",
    "                    state = tf.zeros((batch_size, self.units))\n",
    "                    outputs = []\n",
    "\n",
    "                    # Iterate over timesteps (discretized LTC dynamics)\n",
    "                    for t in range(timesteps):\n",
    "                        x = inputs[:, t, :]  # Current timestep features\n",
    "                        # Compute input modulation\n",
    "                        modulated_input = tf.matmul(x, self.w_input)\n",
    "                        # Update state using Euler-like ODE step\n",
    "                        state_derivative = tf.nn.tanh(modulated_input + tf.matmul(state, self.w_state) + self.bias - state) / self.tau\n",
    "                        state = state + self.dt * state_derivative\n",
    "                        outputs.append(state)\n",
    "                    \n",
    "                    # Return final state (or sequence if return_sequences=True)\n",
    "                    return outputs[-1]  # Shape: (batch, units)\n",
    "\n",
    "            # Build Hybrid Model\n",
    "            def build_hybrid_model(input_shape, n_classes, timesteps=5):\n",
    "                inputs = layers.Input(shape=(timesteps, 100, 100, 4))\n",
    "                \n",
    "                # Spatial Feature Extractor (CNN)\n",
    "                cnn = tf.keras.Sequential([\n",
    "                    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.MaxPooling2D((2, 2)),\n",
    "                    layers.Dropout(0.35),\n",
    "                    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.MaxPooling2D((2, 2)),\n",
    "                    layers.Flatten(),\n",
    "                    layers.Dense(128, activation='relu')\n",
    "                ])\n",
    "                \n",
    "                # Process each timestep with CNN\n",
    "                x = layers.TimeDistributed(cnn)(inputs)  # Output shape: (batch, timesteps, 128)\n",
    "                \n",
    "                # Temporal Dynamics (Liquid Neural Network)\n",
    "                x = LTCLayer(units=64, dt=1.0)(x)  # Custom LTC layer\n",
    "                x = layers.Dropout(0.35)(x)\n",
    "                \n",
    "                # Multi-Time Horizon Prediction\n",
    "                outputs = []\n",
    "                for horizon in [1, 2, 3]:  # Predict 1h, 2h, 3h ahead\n",
    "                    # Shared dense layers for all horizons\n",
    "                    y = layers.Dense(64, activation='relu')(x)\n",
    "                    y = layers.Dense(n_classes, activation='softmax', name=f'output_{horizon}h')(y)\n",
    "                    outputs.append(y)\n",
    "                \n",
    "                model = Model(inputs=inputs, outputs=outputs)\n",
    "                return model\n",
    "\n",
    "            # Initialize Model\n",
    "            model = build_hybrid_model(input_shape=(5, 100, 100, 4), n_classes=3)  # Example: 3 rain classes\n",
    "\n",
    "            # Compile with Multi-Task Learning\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy', tf.keras.metrics.Precision(name='precision')],\n",
    "                loss_weights=[0.3, 0.3, 0.4]  # Weight losses for 1h, 2h, 3h predictions\n",
    "            )\n",
    "            # ======================== NEW LTC + CNN ====================================== #\n",
    "\n",
    "            \n",
    "            # Adjust the data generator\n",
    "            #datagen.fit(x_train_fold)\n",
    "    \n",
    "            #, verbose=0\n",
    "            weights_dir = \"model_weights_by_hour/hour_difference_\" + str(time_difference) + \".weights.h5\"\n",
    "            #load best weights\n",
    "            # Nao realizar este passo para comparação entre metodos de inicialização\n",
    "            try:\n",
    "                model.load_weights(weights_dir)\n",
    "                print(\"Weights loaded\")\n",
    "            except:\n",
    "                print(\"Weights not loaded\")\n",
    "                \n",
    "            history = model.fit(datagen.flow(x_train_fold, y_train_one_hot,batch_size=Batch_len),\n",
    "                                validation_data=(x_val_fold, y_val_one_hot), epochs=NEpochs, callbacks=[model_checkpoint]) \n",
    "            \n",
    "            # Gets training and validation accuracy results\n",
    "            train_accuracy = history.history['accuracy']\n",
    "            val_accuracy = history.history['val_accuracy']\n",
    "            \n",
    "            # Show results\n",
    "            epochs = range(1, len(train_accuracy) + 1)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(epochs, train_accuracy, 'bo', label='Training Accuracy')\n",
    "            plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
    "            plt.title('Training and Validation Accuracy')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "            r=history #necessary, don't touch ;) \n",
    "            n=np.argmax(r.history['accuracy'])\n",
    "            n_l=np.argmin(r.history['loss'])\n",
    "            n_val=np.argmax(r.history['val_accuracy'])\n",
    "            n_vl=np.argmin(r.history['loss'])\n",
    "            n_p=np.argmax(r.history['precision'])\n",
    "            n_vp=np.argmax(r.history['val_precision'])\n",
    "            n_r=np.argmax(r.history['recall'])\n",
    "            n_vr=np.argmax(r.history['val_recall'])\n",
    "    \n",
    "\n",
    "            print(\"   Accuracy Train[\",NEpochs,\"] = \" + \"%.4f\" % np.round(r.history['accuracy'][NEpochs-1],4), \" |  Max Accuracy Train[\",n+1,\"] = \",\\\n",
    "                    \"%.4f\" % np.round(r.history['accuracy'][n],4), \"        |  Accuracy Train[\",n_val+1,\"] = \",\\\n",
    "            \"%.4f\" % np.round(r.history['accuracy'][n_val],4))\n",
    "\n",
    "            print(\"   Accuracy Test[\",NEpochs,\"] = \" + \"%.4f\" % np.round(r.history['val_accuracy'][NEpochs-1],4), \"  |  Validation Accuracy Test[\",n+1,\"] = \",\\\n",
    "                    \"%.4f\" % np.round(r.history['val_accuracy'][n],4), \"  |  Max Valid. Accuracy Test[\",n_val+1,\"] = \",\\\n",
    "            \"%.4f\" % np.round(r.history['val_accuracy'][n_val],4))\n",
    "            \n",
    "            train_accuracies.append(np.round(r.history['accuracy'][n],4))\n",
    "            losses.append(np.round(r.history['loss'][n_l],4))\n",
    "            accuracies.append(np.round(r.history['val_accuracy'][n_val],4))\n",
    "            val_losses.append(np.round(r.history['loss'][n_vl],4))\n",
    "            precisions.append(np.round(r.history['precision'][n_p],4))        \n",
    "            val_precisions.append(np.round(r.history['val_precision'][n_vp],4))        \n",
    "            recalles.append(np.round(r.history['recall'][n_r],4))        \n",
    "            val_recalles.append(np.round(r.history['val_recall'][n_vr],4))        \n",
    "    \n",
    "            data = {\n",
    "                'Epoch' : epochs,\n",
    "                'Accuracy': np.round(r.history['accuracy'],4),\n",
    "                'Loss': np.round(r.history['loss'],4),\n",
    "                'val_accuracy' : np.round(r.history['val_accuracy'],4),\n",
    "                'val_loss' : np.round(r.history['val_loss'],4),      \n",
    "                'precision': np.round(r.history['precision'],4),\n",
    "                'val_precision': np.round(r.history['val_precision'],4),\n",
    "                'recall': np.round(r.history['recall'],4),\n",
    "                'val_recall': np.round(r.history['val_recall'],4)\n",
    "            } \n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Save the DataFrame to the Excel sheet\n",
    "            sheet_name = f'Fold_{fold}'+f'HLayer_{n_HL}'\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            K.clear_session()\n",
    "\n",
    "        train_accuracies_.append(np.mean(train_accuracies))\n",
    "        losses_.append(np.mean(losses))\n",
    "        accuracies_.append(np.mean(accuracies))\n",
    "        val_losses_.append(np.mean(val_losses))  \n",
    "        precisions_.append(np.mean(precisions))  \n",
    "        val_precisions_.append(np.mean(val_precisions))      \n",
    "        recalles_.append(np.mean(recalles))      \n",
    "        val_recalles_.append(np.mean(val_recalles))   \n",
    "\n",
    "    # Show last plot for all Hidden Layers\n",
    "    HiddenLayers = range(NLayer_ini, NLayermax, deltaNLayer)    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(HiddenLayers, train_accuracies_, 'bo', label='Training Accuracy')\n",
    "    plt.plot(HiddenLayers, accuracies_, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Hidden Layer')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    data = {\n",
    "            'HiddenLayer' : HiddenLayers,\n",
    "            'Accuracy': train_accuracies_,\n",
    "            'loss' : losses_,\n",
    "            'val_accuracy' : accuracies_,\n",
    "            'val_loss' : val_losses_,\n",
    "            'precision' : precisions_,\n",
    "            'val_precision' : val_precisions_,\n",
    "            'recall' : recalles_,\n",
    "            'val_recall' : val_recalles_\n",
    "        }                                            \n",
    "    df = pd.DataFrame(data) \n",
    "    # Save the DataFrame to the Excel sheet\n",
    "    sheet_name = f'Fold_HL_Average{NLayermax}'\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('\\n --- KeyboardInterrupt detected ---')\n",
    "finally:\n",
    "    print(\"\")\n",
    "    mean_Train_accuracy = np.mean(train_accuracies)\n",
    "    print(f\"\\nAverage of Train Accuracy = {n_folds} folds: {mean_Train_accuracy * 100:.2f}%\")\n",
    "    print(\"\")\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"\\nAverage of Validation Accuracy = {n_folds} folds: {mean_accuracy * 100:.2f}%\")\n",
    "    print(\"\")\n",
    "    print(\"train_accuracies_\",train_accuracies_)\n",
    "    print(\"losses_\",losses_)\n",
    "    print(\"accuracies_\",accuracies_)\n",
    "    print(\"precisions_\",precisions_)  \n",
    "    print(\"val_precisions_\",val_precisions_)      \n",
    "    print(\"recalles_\",recalles_)      \n",
    "    print(\"val_recalles_\",val_recalles_) \n",
    "    print(\"\")\n",
    "\n",
    "    model.summary()\n",
    "   \n",
    "    #Close the Writer\n",
    "    writer.close()#\n",
    "    \n",
    "    #move last sheet to first position\n",
    "    # Carrega o workbook\n",
    "    wb = load_workbook(xlsFileName)\n",
    "    last_sheet_name = wb.sheetnames[-1]\n",
    "    sheet_to_move = wb[last_sheet_name]\n",
    "    wb.remove(sheet_to_move)\n",
    "    wb._sheets = [sheet_to_move] + wb._sheets  # Usando _sheets para manipular diretamente a lista de folhas\n",
    "    wb.save(xlsFileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ad862-67fd-480b-80e8-f4709be12429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86ee43-acb0-4040-bc36-fbc3326e4e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f9fe3-c6a6-49fd-846b-c6280dafa6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34936586-176f-4339-aaac-085b38dc7cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvteste39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
